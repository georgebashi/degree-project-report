This file was created with JabRef 2.2.
Encoding: ISO8859_1

@INPROCEEDINGS{Allamanche2001,
  author = {Eric Allamanche},
  title = {Content-based Identification of Audio Material Using {MPEG}-7 Low
	Level Description},
  booktitle = {ISMIR},
  year = {2001},
  owner = {george},
  pdf = {Allamanche2001.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Arenas-Garcia2006,
  author = {Jeronimo Arenas-Garcia and J. Larsen and L. K. Hansen and A. Meng},
  title = {Optimal Filtering of Dynamics in short-time Features for Music Organization},
  booktitle = {ISMIR},
  year = {2006},
  owner = {george},
  pdf = {Arenas-Garcia2006.pdf},
  timestamp = {2007.03.20}
}

@TECHREPORT{Aucouturier2006,
  author = {Jean-Julien Aucouturier and Anthony Beurive and Florian Plenge and
	Christian Sailer},
  title = {Public Report of {WP}3: ``{B}rowsing''},
  institution = {Semantic HIFI},
  year = {2006},
  month = {nov},
  pdf = {Aucouturier2006.pdf}
}

@INPROCEEDINGS{Aucouturier2004,
  author = {Jean-Julien Aucouturier and François Pachet},
  title = {Tools and Architecture for the Evaluation of Similarity Measures:
	Case Study of Timbre Similarity},
  booktitle = {ISMIR},
  year = {2004},
  owner = {george},
  pdf = {Aucouturier2004.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Aucouturier2003,
  author = {Jean-Julien Aucouturier and François Pachet},
  title = {Scaling Up Music Playlist Generation},
  booktitle = {Proceedings of {IEEE} {I}nternational {C}onference on {M}ultimedia
	and {E}xpo ({ICME})},
  year = {2003},
  month = aug,
  __markedentry = {[george]},
  abstract = {The issue of generating automatically sequences of music titles that
	satisfy arbitrary criteria such as user preferences has gained interest
	recently, because of the numerous applications in the field of Electronic
	Music Distribution. All the approaches proposed so far suffer from
	two main drawbacks: reduced expressiveness and incapacity to handle
	large music catalogues. We present in this paper a system that is
	able to produce automatically music playlists out of large, real
	catalogues (up to 200,000 titles), and that can handle arbitrarily
	complex criteria. We describe the basic algorithm and its adaptation
	to playlist generation, and report on experiments performed in the
	context of the European project Cuidado.},
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/669083.html},
  pdf = {Aucouturier2003.pdf}
}

@ARTICLE{Aucouturier2003a,
  author = {Jean-Julien Aucouturier and François Pachet},
  title = {Representing Musical Genre: A State of the Art},
  journal = {Journal of New Music Research},
  year = {2003},
  volume = {32},
  pages = {83--93},
  number = {1},
  owner = {george},
  pdf = {Aucouturier2003a.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Aucouturier2002,
  author = {Jean-Julien Aucouturier and François Pachet},
  title = {Music Similarity Measures: What's the use?},
  booktitle = {ISMIR},
  year = {2002},
  owner = {george},
  pdf = {Aucouturier2002.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Aucouturier2002a,
  author = {Jean-Julien Aucouturier and François Pachet},
  title = {Finding Songs That Sound The Same},
  booktitle = {IEEE Benelux Workshop on Model based Processing and Coding of Audio},
  year = {2002},
  month = {nov},
  __markedentry = {[george]},
  abstract = {A crucial dimension of Content-based music management systems is their
	ability to compute automatically similarities between music titles.
	We propose a technique that allows users to find music titles that
	sound similar to songs they like. The technique relies on a modelling
	of the timbral characteristics of a music signal by distributions
	of Cepstrum coefficients. The resulting models are then compared
	to yield a similarity measure. The paper describes the algorithm,
	and proposes an evaluation of the quality of the extracted similarity
	measure. Additionally, we illustrate the use of this measure in two
	Electronic Music Distribution applications developed in the context
	of the European project Cuidado.},
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/671525.html},
  owner = {george},
  pdf = {Aucouturier2002a.pdf},
  review = {The main drawback of these approaches is that they are essentially
	content-blind; the music itself is ignored, and only users tastes
	are considered. The resulting recommendations are therefore at best
	superficially relevant. In this paper we propose to go further in
	the direction of content-based extraction by computing automatically
	music similarities between music titles based on their global timbral
	quality. The motivation for such an endeavour is two fold. First,
	although it is difficult to define precisely music taste, it is quite
	obvious that music taste is often correlated with timbre. Some sounds
	are pleasing to listeners, other are not. Some timbres are specific
	to music periods (e.g. the sound of Chick Corea playing on an electric
	piano), others to musical configurations (e.g. the sound of a symphonic
	orchestra). In any case, listeners are sensitive to timbre, at least
	in a global manner. There has been a large quantity of work about
	timbre. However most of them have focussed on monophonic simple sound
	samples, aiming at Instrument Recognition ([7]), i.e. identifying
	if a note is being played on a trumpet or a clarinet. Here, we are
	concerned with full polyphonic music and complex instrumental textures,
	for which we want to extract a global timbre description. Among related
	work in this domain, Automatic Genre Classification ([8]) tries to
	categorize music titles into genre classes by looking at spectral
	or temporal signal features. In this approach, the tested song?s
	timbre is matched against pre-computed models of each possible genre.
	Each genre model averages the timbre of a large number of songs that
	are known to belong to this genre. There is no matching from one
	song to another, but rather from one song to a group of songs. Music
	title identification ([9]) deals with identifying the title and artist
	of an arbitrary music signal. This is done by comparing the unlabelled
	signal?s features to a database containing the features of all possible
	identified songs. In this case, the matching is done from one song
	to another, but the system only looks for exact matches, not for
	similarity. Our system performs approximate matching of one song
	to another. Similar timbres must be represented by close "points"
	in a multi- dimensional feature space, and, conversely, close points
	in this space should correspond to similar timbres. At the same time,
	since we do not want to take into account the melodic content of
	the songs, the feature set should be relatively independent of pitch.},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Basili2004,
  author = {Roberto Basili and Alfredo Serafini and Armando Stellato},
  title = {Classification of musical genre: a machine learning approach},
  booktitle = {ISMIR},
  year = {2004},
  owner = {george},
  pdf = {Basili2004.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Bello2000,
  author = {Juan Pablo Bello and Giuliano Monti and Mark Sandler},
  title = {Techniques for Automatic Music Transcription},
  booktitle = {ISMIR},
  year = {2000},
  owner = {george},
  pdf = {Bello2000.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Bello2005,
  author = {Juan P. Bello and Jeremy Pickens},
  title = {A Robust Mid-level Representation for Harmonic Content in Music Signals},
  booktitle = {ISMIR},
  year = {2005},
  owner = {george},
  pdf = {Bello2005.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Berenzweig2003,
  author = {Adam Berenzweig and Beth Logan and Daniel P.W. Ellis and Brian Whitman},
  title = {A Large-Scale Evaluation of Acoustic and Subjective Music Similarity
	Measures},
  booktitle = {ISMIR},
  year = {2003},
  __markedentry = {[george]},
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/650651.html},
  pdf = {Berenzweig2003.pdf}
}

@INBOOK{Bourque2004,
  chapter = {2},
  title = {Guide to the software engineering body of knowledge},
  publisher = {Version},
  year = {2004},
  editor = {Alain Abran and James W. Moore},
  author = {Pierre Bourque and Robert Dupuis},
  owner = {george},
  timestamp = {2007.04.12}
}

@INPROCEEDINGS{Bray2005,
  author = {Stuart Bray and George Tzanetakis},
  title = {Distributed Audio Feature Extraction for Music},
  booktitle = {ISMIR},
  year = {2005},
  owner = {george},
  pdf = {Bray2005.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Bruderer2006,
  author = {Michael J. Bruderer and M. F. McKinney and A. Kohlrausch},
  title = {Perception of Structure in Popular Music},
  booktitle = {ISMIR},
  year = {2006},
  owner = {george},
  pdf = {Bruderer2006.pdf},
  timestamp = {2007.03.20}
}

@ARTICLE{Byrd2001,
  author = {Donald Byrd and Tim Crawford},
  title = {Problems of music information retrieval in the real world},
  journal = {Information Processing and Management},
  year = {2001},
  volume = {38},
  pages = {249--272}
}

@INPROCEEDINGS{Cahill2006,
  author = {Margaret Cahill and Donncha Ó Maidín},
  title = {Assessing the Performance of Melodic Similarity Algorithms Using
	Human Judgments of Similarity},
  booktitle = {ISMIR},
  year = {2006},
  __markedentry = {[george]},
  abstract = {This paper outlines a project to identify reliable algorithms for
	measuring melodic similarity by using melodies extracted from a piece
	of music in Theme and Variations form, for which human judgements
	of similarity have been gathered.},
  owner = {george},
  pdf = {Cahill2006.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Cahill2005,
  author = {Margaret Cahill and Donncha Ó Maidín},
  title = {Melodic Similarity Algorithms -- Using Similarity Ratings for Development
	and Early Evaluation},
  booktitle = {ISMIR},
  year = {2005},
  abstract = {This paper focuses on gathering similarity ratings for use in the
	construction, optimization and evaluation of melodic similarity algorithms.
	The approach involves con ducting listening experiments to gather
	these ratings for a piece in Theme and Variation form.},
  owner = {george},
  pdf = {Cahill2005.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Casagrande2005,
  author = {Norman Casagrande and Douglas Eck and Balázs Kégl},
  title = {Frame-Level Audio Feature Extraction Using {A}da{B}oost},
  booktitle = {ISMIR},
  year = {2005},
  owner = {george},
  pdf = {Casagrande2005.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Chai2000,
  author = {Wei Chai and Barry Vercoe},
  title = {Using User Models in Music Information Retrieval Systems},
  booktitle = {ISMIR},
  year = {2000},
  abstract = {Most websites providing music services only support category-based
	browsing and/or text-based searching. There has been some research
	to improve the interface either for pull applications, e.g. query-by-humming
	systems, or for push applications, e.g. collaborative-filtering-based
	or feature-based music recommendation systems. However, for content-based
	search or feature-based filtering systems, one important problem
	is to describe music by its parameters or features, so that search
	engines or information filtering agents can use them to measure the
	similarity of the target (user's query or preference) and the candidates.
	MPEG7 (formally called "Multimedia Content Description Interface")
	is an international standard, which describes the multimedia content
	data to allow universal indexing, retrieval, filtering, control,
	and other activities supported by rich metadata. However, the metadata
	about the multimedia content itself are still insufficient, because
	many features of multimedia content are quite perceptual and user-dependent.
	For example, emotional features are very important for multimedia
	retrieval, but they are hard to be described by a universal model
	since different users may have different emotional responses to the
	same multimedia content. We therefore turn to user modeling techniques
	and representations to describe the properties of each user, so that
	the retrieval will be more accurate. Besides, user modeling can be
	used to reduce the search space, make push service easier and improve
	the user interface.},
  owner = {george},
  pdf = {Chai2000.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Cooper2002,
  author = {Matthew Cooper and Jonathan Foote},
  title = {Automatic Music Summarization via Similarity Analysis},
  booktitle = {ISMIR},
  year = {2002},
  abstract = {We present methods for automatically producing summary excerpts or
	thumbnails of music. To find the most representative excerpt, we
	maximize the average segment similarity to the entire work. After
	window-based audio parameterization, a quantitative similarity measure
	is calculated between every pair of windows, and the results are
	embedded in a 2-D similarity matrix. Summing the similarity matrix
	over the support of a segment results in a measure of how similar
	that segment is to the whole. This measure is maximized to find the
	segment that best represents the entire work. We discuss variations
	on the method, and present experimental results for orchestral music,
	popular songs, and jazz. These results demonstrate that the method
	finds significantly representative excerpts, using very few assumptions
	about the source audio.},
  owner = {george},
  pdf = {Cooper2002.pdf},
  timestamp = {2007.03.20}
}

@ARTICLE{Downie2003,
  author = {J. Stephen Downie},
  title = {Music Information Retrieval},
  journal = {Annual Review of Information Science and Technology},
  year = {2003},
  volume = {37},
  pages = {295--340},
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/462854.html}
}

@INPROCEEDINGS{Fingerhut2004,
  author = {Michael Fingerhut},
  title = {Music Information Retrieval, or how to search for (and maybe find)
	music and do away with incipits},
  booktitle = {IAML-IASA Congress},
  year = {2004},
  month = aug,
  location = {Oslo (Norway)}
}

@INPROCEEDINGS{Gouyon2000,
  author = {Fabien Gouyon and François Pachet and Olivier Delerue},
  title = {On the use of {Z}ero-{C}rossing {R}ate for an application of classification
	of percussive sounds},
  booktitle = {COST G-6 Conference on Digital Audio Effects (DAFX)},
  year = {2000},
  address = {Verona, Italy},
  owner = {george},
  timestamp = {2007.04.15}
}

@ARTICLE{Hainsworth2004,
  author = {Stephen W. Hainsworth and Malcolm D. Macleod},
  title = {The Automated Music Transcription Problem},
  year = {2004},
  __markedentry = {[george]},
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/636235.html}
}

@MISC{Hunt2007,
  author = {Johanna Hunt},
  title = {Introduction to Research Methods (Lecture Slides)},
  howpublished = {Online},
  year = {2007},
  owner = {george},
  pdf = {Hunt2007.pdf},
  timestamp = {2007.04.10}
}

@INPROCEEDINGS{Itoyama2006,
  author = {Katsutoshy Itoyama and T. Kitahara and K. Komatani and T. Ogata and
	H. G. Okuno},
  title = {Automatic Feature Weighting in Automatic Transcription of Specified
	Part in Polyphonic Music},
  booktitle = {ISMIR},
  year = {2006},
  abstract = {We studied the problem of automatic music transcription (AMT) for
	polyphonic music. AMT is an important task for music information
	retrieval because AMT results enable retrieving musical pieces, high-level
	annotation, demixing, etc. We attempted to transcribe a part played
	by an instrument specified by users (specified part tracking). Only
	two timbre models are required in the specified part tracking to
	identify the specified musical instrument even when the number of
	instruments increases. This transcription is formulated into a time-series
	classification problem with multiple features. We furthermore attempted
	to automatically estimate weights of the features, because the importance
	of these features varies for each musical signal. We estimated quasi-optimal
	weights of the features using a genetic algorithm for each musical
	signal. We tested our AMT system using trio stereo musical signals.
	Accuracies with our feature weighting method were 69.8% on average,
	whereas those without feature weighting were 66.0%.},
  owner = {george},
  pdf = {Itoyama2006.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Izmirli2005,
  author = {Ozgur Izmirli},
  title = {Tonal Similarity from Audio Using a Template Based Attractor Model},
  booktitle = {ISMIR},
  year = {2005},
  owner = {george},
  pdf = {Izmirli2005.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Jacobson2006,
  author = {Kurt Jacobson},
  title = {A Multifaceted Approach to Music Similarity},
  booktitle = {ISMIR},
  year = {2006},
  abstract = {Previous work has explored the concept of music similarity measures
	and a variety of methods have been proposed for calculating such
	measures. This paper describes a system for music similarity which
	attempts to model and compare some of the more musically salient
	features of a set of audio signals. A model for timbre and a model
	for rhythm are implemented directly from previous work, and a model
	for song structure is developed. The different models are weighted
	and combined to provide an overall music similarity measure. The
	system is tested on a small set of popular music files spanning eleven
	different genres. The system is tuned to estimate genre boundaries
	using multidimensional scaling - a technique that allows for quick
	visualization of similarity data. An "automatic DJ" application,
	that generates playlists based on the music similarity models, serves
	as a subjective evaluation for the system.},
  owner = {george},
  pdf = {Jacobson2006.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Jones2004,
  author = {Steve Jones and Sally Jo Cunningham and Matt Jones},
  title = {Organizing digital music for use: an examination of personal music
	collections},
  booktitle = {ISMIR},
  year = {2004},
  owner = {george},
  pdf = {Jones2004.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Karydis2005,
  author = {Ioannis Karydis and Alexandros Nanopoulos and Apostolos Papadopoulos
	and Dimitrios Katsaros and Yannis Manolopoulos},
  title = {Content-Based Music Information Retrieval in Wireless Ad-Hoc Networks},
  booktitle = {ISMIR},
  year = {2005},
  owner = {george},
  pdf = {Karydis2005.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Kim2006,
  author = {Youngmoo E. Kim and D. S. Williamson and S. Pilli},
  title = {Understanding and Quantifying the ``Album Effect'' in Artist Identification},
  booktitle = {ISMIR},
  year = {2006},
  __markedentry = {[george]},
  abstract = {Recent systems for automatically identifying the performing artist
	from the acoustic signal of music have demonstrated reasonably high
	accuracy when discriminating between hundreds of known artists. A
	well-documented issue, however, is that the performance of these
	systems degrades when music from different albums is used for training
	and evaluation. Conversely, accuracy improves when systems are trained
	and evaluated using music from the same album. This performance characteristic
	has been labeled the ``album effect''. The unfortunate corollary
	to this result is that the classification results of these systems
	are based not entirely on the music itself, but on other audio features
	common to the album that may be unrelated to the underlying music.
	We hypothesize that one of the primary reasons for this phenomenon
	is the production process of commercial recordings, specifically,
	post-production. Understanding the primary aspects of post-production,
	we can attempt to model its effect on the acoustic features used
	for classification. quantifying and accounting for this transformation,
	we hope to improve future systems for automatic artist identification.},
  owner = {george},
  pdf = {Kim2006.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Klapuri2001,
  author = {A. Klapuri and A. Eronen and J. Seppanen and T. Virtanen},
  title = {Automatic Transcription of Music},
  booktitle = {Symposium on Stochastic Modelling of Music},
  year = {2001},
  month = {22 } # oct,
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/697980.html},
  location = {Ghent (Belgium)}
}

@INPROCEEDINGS{Klapuri2003,
  author = {Anssi P. Klapuri},
  title = {Multiple Fundamental Frequency Estimation Based on Harmonicity and
	Spectral Smoothness},
  booktitle = {IEEE Transactions on Speech and Audio Processing},
  year = {2003},
  volume = {11},
  number = {6},
  pages = {804--816},
  month = {Nov},
  owner = {george},
  timestamp = {2007.04.18}
}

@INPROCEEDINGS{Lidy2005,
  author = {Thomas Lidy and Andreas Rauber},
  title = {Evaluation of Feature Extractors and Psycho-Acoustic Transformations
	for Music Genre Classification},
  booktitle = {ISMIR},
  year = {2005},
  abstract = {We present a study on the importance of psycho-acoustic transformations
	for effective audio feature calculation. From the results, both crucial
	and problematic parts of the algorithm for Rhythm Patterns feature
	extraction are identified. We furthermore introduce two new feature
	rep- resentations in this context: Statistical Spectrum Descrip-
	tors and Rhythm Histogram features. Evaluation on both the individual
	and combined feature sets is accomplished through a music genre classification
	task, involving 3 ref- erence audio collections. Results are compared
	to pub- lished measures on the same data sets. Experiments con- firmed
	that in all settings the inclusion of psycho-acoustic transformations
	provides significant improvement of clas- sification accuracy.},
  owner = {george},
  pdf = {Lidy2005.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Liu2003,
  author = {Dan Liu and Lie Lu and Hong-Jiang Zhang},
  title = {Automatic mood detection from acoustic music data},
  booktitle = {ISMIR},
  year = {2003},
  __markedentry = {[george]},
  abstract = {Music mood describes the inherent emotional meaning of a music clip.
	It is helpful in music understanding, music search and some music-related
	applications. In this paper, a hierarchical framework is presented
	to automate the task of mood detection from acoustic music data,
	by following some music psychological theories in western cultures.
	Three feature sets, intensity, timbre and rhythm, are extracted to
	represent the characteristics of a music clip. Moreover, a mood tracking
	approach is also presented for a whole piece of music. Experimental
	evaluations indicate that the proposed algorithms produce satisfactory
	results.},
  owner = {george},
  pdf = {Liu2003.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Logan2002,
  author = {Beth Logan},
  title = {Content-Based Playlist Generation: Exploratory Experiments},
  booktitle = {ISMIR},
  year = {2002},
  owner = {george},
  pdf = {Logan2002.pdf},
  timestamp = {2007.03.20}
}

@TECHREPORT{Logan2001,
  author = {Beth Logan and Ariel Salomon},
  title = {A Content-Based Music Similarity Function},
  institution = {Cambridge Research Laboratory},
  year = {2001},
  month = {jun},
  __markedentry = {[george]},
  abstract = {We present a method to compare songs based solely on their audio content.
	Our technique forms a signature for each song based on K-means clustering
	of spectral features. The signatures can then be compared using the
	Earth Mover?s Distance [14] which allows comparison of histograms
	with disparate bins. Preliminary objective and subjective results
	on a database of over 8000 songs are encouraging. For 20 songs judged
	by two users, on average 2.5 out of the top 5 songs returned were
	judged similar. We also found that our measure is robust to simple
	corruption of the audio signal and that meaningful visualizations
	of the data are possible using this similarity measure.},
  owner = {george},
  pdf = {Logan2001.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Logan2001a,
  author = {Beth Logan and Ariel Salomon},
  title = {A Music Similarity Function Based on Signal Analysis},
  booktitle = {International Conference on Multimedia \& Expo (ICME)},
  year = {2001},
  __markedentry = {[george]},
  owner = {george},
  pdf = {Logan2001a.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Mardirossian2006,
  author = {Arpi Mardirossian and E. Chew},
  title = {Music Summarization Via Key Distributions: Analyses of Similarity
	Assessment Across Variations},
  booktitle = {ISMIR},
  year = {2006},
  abstract = {This paper presents a computationally efficient method for quantifying
	the degree of tonal similarity between two pieces of music. The properties
	we examine are key frequencies and average time in key, and we propose
	two metrics, based on the L1 and L2 norms, for quantifying similarity
	using these descriptors. The methods are applied to 711 classical
	themes and variations over 71 variation sets by 10 composers of different
	genres. Quantile-quantile plots and the Kolmogorov-Smirnov measure
	show that the proposed metrics exhibit strongly distinct behaviour
	when assessing pieces from the same variation set, and those that
	are not. Comparisons across variation sets by the same composer,
	and comparisons of pieces by different composers although result
	in similar distributions, are derived from fundamentally different
	underlying distributions, according to the K-S measure. We present
	probabilistic analyses of the two methods based on the distributions
	derived empirically. When the discrimination threshold is set at
	55, the probabilities of Type I and Type II errors are 18.41% and
	20.56% respectively for Method 1, and 15.72% and 22.94% respectively
	for Method 2. Method 1 has a success rate of 99.48% when labeling
	pieces as dissimilar (not from the same variation set), while the
	corresponding rate for Method 2 is 99.45%.},
  owner = {george},
  pdf = {Mardirossian2006.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{McEnnis2005,
  author = {Daniel McEnnis and Cory McKay and Ichiro Fujinaga and Philippe Depalle},
  title = {j{A}udio: An Feature Extraction Library},
  booktitle = {ISMIR},
  year = {2005},
  abstract = {jAudio is a new framework for feature extraction designed to eliminate
	the duplication of effort in calculating features from an audio signal.
	This system meets the needs of MIR researchers by providing a library
	of analysis algorithms that are suitable for a wide array of MIR
	tasks. In order to provide these features with a minimal learning
	curve, the system implements a GUI that makes the process of selecting
	desired features straight forward. A command-line interface is also
	provided to manipulate jAudio via scripting. Furthermore, jAudio
	provides a unique method of handling multidimensional features and
	a new mechanism for dependency handling to prevent duplicate calculations.
	The system takes a sequence of audio files as input. In the GUI,
	users select the features that they wish to have extracted - letting
	jAudio take care of all dependency problems - and either execute
	directly from the GUI or save the settings for batch processing.
	The output is either an ACE XML file or an ARFF file depending on
	the user?s preference.},
  owner = {george},
  pdf = {McEnnis2005.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Muellensiefen2004,
  author = {Daniel Müllensiefen and Klaus Frieler},
  title = {Optimizing Measures Of Melodic Similarity For The Exploration Of
	A Large Folk Song Database},
  booktitle = {ISMIR},
  year = {2004},
  abstract = {This investigation aims at finding an optimal way of measuring the
	similarity of melodies. The applicability for an automated analysis
	and classification was tested on a folk song collection from Luxembourg
	that had been thoroughly analysed by an expert ethnomusicologist.
	Firstly a systematization of the currently available approaches to
	similarity measurements of melodies was done. About 50 similarity
	measures were implemented which differ in the way of transforming
	musical data and in the computational algorithms. Three listener
	experiments were conducted to compare the performance of the different
	measures to human experts? ratings. Then an optimized model was obtained
	by using linear regression, which combines the output of several
	measures representing different musical dimensions. The performance
	of this optimized measure was compared with the classification work
	of a human ethnomusicologist on a collection of 577 Luxembourg folksongs.},
  owner = {george},
  pdf = {Muellensiefen2004.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Novello2006,
  author = {Alberto Novello and M. F. McKinney and A. Kohlrausch},
  title = {Perceptual Evaluation of Music Similarity},
  booktitle = {ISMIR},
  year = {2006},
  __markedentry = {[george]},
  abstract = {This paper presents an empirical method for assessing music similarity
	on a set of stimuli using triadic comparisons in a balanced incomplete
	block design. We first evaluated the consistency of subjects in their
	rankings and then the con cordance across subjects. The concordance
	was also evalu ated for different subject populations to assess the
	influence of experience of the subject with the musical material.
	We finally analysed subjects? ranking by the means of multidimensional
	scaling. Similarity judgments were found to be rather concordant
	across subjects. Significant differences between musicians and non-musicians
	and between subjects being familiar or non-familiar with the music
	were found for a small number of cases. Multidimensional scaling
	reveals a proximity of songs belonging to the same genre, congruent
	with the idea of genre being a perceptual dimension in subjects?
	similarity ranking.},
  owner = {george},
  pdf = {Novello2006.pdf},
  timestamp = {2007.03.20}
}

@ARTICLE{Pachet1999,
  author = {François Pachet and Pierre Roy and Daniel Cazaly},
  title = {A Combinatorial Approach to Content-based Music Selection},
  journal = {IEEE MultiMedia},
  year = {1999},
  volume = {7},
  pages = {44--51},
  number = {1},
  month = {--},
  citeseercitationcount = {0},
  citeseerurl = {http://citeseer.ist.psu.edu/266121.html},
  owner = {george},
  pdf = {Pachet1999.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Pachet2004,
  author = {François Pachet and Aymeric Zils},
  title = {Automatic extraction of music descriptors from acoustic signals},
  booktitle = {ISMIR},
  year = {2004},
  __markedentry = {[george]},
  abstract = {High-Level music descriptors are key ingredients for music information
	retrieval systems. Although there is a long tradition in extracting
	information from acoustic signals, the field of music information
	extraction is largely heuristic in nature. We present here a heuristic-based
	generic approach for extracting automatically high-level music descriptors
	from acoustic signals. This approach is based on Genetic Programming,
	used to build relevant features as functions of mathematical and
	signal processing operators. The search of relevant features is guided
	by specialized heuristics that embody knowledge about the signal
	processing functions built by the system. Signal processing patterns
	are used in order to control the general processing methods. In addition,
	rewriting rules are introduced to simplify overly complex expressions,
	and a caching system further reduces the computing cost of each cycle.
	Finally, the features build by the system are combined into an optimized
	machine learning descriptor model, and an executable program is generated
	to compute the model on any audio signal. In this paper, we describe
	the overall system and compare its results against traditional approaches
	in musical feature extraction Ã  la Mpeg7.},
  owner = {george},
  pdf = {Pachet2004.pdf},
  timestamp = {2007.03.20}
}

@PHDTHESIS{Pampalk2006,
  author = {Elias Pampalk},
  title = {Computational Models of Music Similarity and their Application in
	Music Information Retrieval},
  school = {Vienna University of Technology},
  year = {2006},
  month = {3},
  __markedentry = {[george]},
  abstract = {This thesis aims at developing techniques which support users in accessing
	and discovering music. The main part consists of two chapters. Chapter
	2 gives an introduction to computational models of music simi- larity.
	The combination of different approaches is optimized and the largest
	evaluation of music similarity measures published to date is presented.
	The best combination performs significantly better than the baseline
	approach in most of the evaluation categories. A particular effort
	is made to avoid overfitting. To cross-check the results from the
	evaluation based on genre classification a listening test is conducted.
	The test confirms that genre- based evaluations are suitable to efficiently
	evaluate large parameter spaces. Chapter 2 ends with recommendations
	on the use of similarity measures. Chapter 3 describes three applications
	of such similarity measures. The first application demonstrates how
	music collections can be organized and vi- sualized so that users
	can control the aspect of similarity they are interested in. The
	second application demonstrates how music collections can be orga-
	nized hierarchically into overlapping groups at the artist level.
	These groups are summarized using words from web pages associated
	with the respective artists. The third application demonstrates how
	playlists can be generated which require minimum user input.},
  owner = {george},
  pdf = {Pampalk2006.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Pampalk2004,
  author = {Elias Pampalk},
  title = {A {M}atlab Toolbox to Compute Music Similarity from Audio},
  booktitle = {ISMIR},
  year = {2004},
  abstract = {A Matlab toolbox implementing music similarity measures for audio
	is presented. The implemented measures focus on aspects related to
	timbre and periodicities in the signal. This paper gives an overview
	of the implemented functions. In particular, the basics of the similarity
	mea sures are reviewed and some visualizations are discussed.},
  owner = {george},
  pdf = {Pampalk2004.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Pampalk2005,
  author = {Elias Pampalk and Arthur Flexer and Gerhard Widmer},
  title = {Improvements of Audio-Based Music Similarity and Genre Classificaton},
  booktitle = {ISMIR},
  year = {2005},
  abstract = {Audio-based music similarity measures can be applied to automatically
	generate playlists or recommendations. In this paper spectral similarity
	is combined with comple- mentary information from fluctuation patterns
	including two new descriptors derived thereof. The performance is
	evaluated in a series of experiments on four music col- lections.
	The evaluations are based on genre classifica- tion, assuming that
	very similar tracks belong to the same genre. The main findings are
	that, (1) although the im- provements are substantial on two of the
	four collections our extensive experiments confirm earlier findings
	that we are approaching the limit of how far we can get using sim-
	ple audio statistics. (2) We have found that evaluating sim- ilarity
	through genre classification is biased by the music collection (and
	genre taxonomy) used. Furthermore, (3) in a cross validation no pieces
	from the same artist should be in both training and test set.},
  owner = {george},
  pdf = {Pampalk2005.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Pampalk2005a,
  author = {Elias Pampalk and Tim Pohle and Gerhard Widmer},
  title = {Dynamic Playlist Generation Based On Skipping Behavior},
  booktitle = {ISMIR},
  year = {2005},
  __markedentry = {[george]},
  citeseercitationcount = {0},
  citeseerurl = {732589},
  owner = {george},
  pdf = {Pampalk2005a.pdf},
  timestamp = {2007.03.20}
}

@MASTERSTHESIS{Park2004,
  author = {Tae Hong Park},
  title = {Towards Automatic Musical Instrument Timbre Recognition},
  school = {Princeton University},
  year = {2004},
  month = {Nov},
  owner = {george},
  timestamp = {2007.04.16}
}

@INPROCEEDINGS{Pauws2002,
  author = {Steffen Pauws and Berry Eggen},
  title = {PATS: Realization and User Evaluation of an Automatic Playlist Generator},
  booktitle = {ISMIR},
  year = {2002},
  abstract = {A means to ease selecting preferred music referred to as Personalized
	Automatic Track Selection (PATS) has been developed. PATS generates
	playlists that suit a particular context- of-use, that is, the real-world
	environment in which the music is heard. To create playlists, it
	uses a dynamic clustering method in which songs are grouped based
	on their attribute similarity. The similarity measure selectively
	weighs attribute-values, as not all attribute-values are equally
	important in a context-of-use. An inductive learning algorithm is
	used to reveal the most important attribute-values for a context-of-use
	from preference feedback of the user. In a controlled user experiment,
	the quality of PATS- compiled and randomly assembled playlists for
	jazz music was assessed in two contexts-of-use. The quality of the
	randomly assembled playlists was used as base-line. The two contexts-of-use
	were ?listening to soft music? and ?listening to lively music?. Playlist
	quality was measured by precision (songs that suit the context-of-use),
	coverage (songs that suit the context-of-use but that were not already
	contained in previous playlists) and a rating score. Results showed
	that PATS playlists contained increasingly more preferred music (increasingly
	higher precision), covered more preferred music in the collection
	(higher coverage), and were ratedhigher than randomly assembled playlists.},
  owner = {george},
  pdf = {Pauws2002.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Pauws2006,
  author = {Steffen Pauws and W. Verhaegh and M. Vossen},
  title = {Fast Generation of Optimal Music Playlists using Local Search},
  booktitle = {ISMIR},
  year = {2006},
  __markedentry = {[george]},
  abstract = {We present an algorithm for use in an interactive music sys- tem that
	automatically generates music playlists that fit the music preferences
	given by a user. To this end, we introduce a formal model, define
	the problem of automatic playlist generation (APG) and indicate its
	NP-hardness. We use a local search (LS) procedure based on simulated
	annealing (SA) to solve the APG problem. In order to employ this
	LS procedure, we introduce an optimization variant of the APG problem,
	which includes the definition of penalty functions and a neighborhood
	structure. To improve upon the per- formance of the standard SA algorithm,
	we incorporated three heuristics referred to as song domain reduction,
	par- tial constraint voting, and two-level neighborhood structure.
	In tests, LS performed better than a constraint satisfaction (CS)
	solution in terms of run time, scalability and playlist quality.},
  owner = {george},
  pdf = {Pauws2006.pdf},
  timestamp = {2007.03.20}
}

@TECHREPORT{Peeters2004,
  author = {Geoffroy Peeters},
  title = {A large set of audio features for sound description (similarity and
	classification) in the {CUIDADO} project},
  institution = {Ircam, Analysis/Synthesis Team},
  year = {2004},
  address = {1 pl. Igor Stravinsky, 75004, Paris, France},
  month = {Apr},
  owner = {george},
  timestamp = {2007.04.18}
}

@UNPUBLISHED{Schnitzer2003,
  author = {Dominic Schnitzer},
  title = {Mo{C} --- {M}aster of {C}elebration: Automatic Playlist Generation},
  month = {26 } # aug,
  year = {2003}
}

@INPROCEEDINGS{Schubert2004,
  author = {Emery Schubert and Joe Wolfe and Alex Tarnopolsky},
  title = {Spectral centroid and timbre in complex, multiple instrumental textures},
  booktitle = {8th International Conference on Music Perception \& Cognition},
  year = {2004},
  editor = {S D Lipscomb and R Ashley and R O Gjerdingen and P Webster},
  pages = {654--657},
  address = {Evanston, IL},
  owner = {george},
  timestamp = {2007.04.16}
}

@PHDTHESIS{Schwarz2004,
  author = {Diemo Schwarz},
  title = {Data-Driven Concatenative Sound Synthesis},
  school = {Ircam},
  year = {2004},
  owner = {george},
  pdf = {Schwarz2004.pdf},
  timestamp = {2007.04.04}
}

@BOOK{Sides1998,
  title = {How to Write \& Present Technical Information},
  publisher = {The Oryx Press},
  year = {1998},
  author = {Charles Sides},
  edition = {3},
  owner = {george},
  timestamp = {2007.04.10}
}

@MISC{Siegrist2007,
  author = {Kyle Siegrist},
  title = {Virtual Laboratories in Probability and Statistics: Variance and
	Higher Moments},
  howpublished = {Online},
  month = {18 Jan},
  year = {2007},
  note = {\url{http://www.math.uah.edu/stat/expect/Variance.xhtml}},
  owner = {george},
  timestamp = {2007.04.16}
}

@INPROCEEDINGS{Stenzel2005,
  author = {Richard Stenzel and Thomas Kamps},
  title = {Improving Content-Based Similarity Measures by Training a Collaborative
	Model},
  booktitle = {ISMIR},
  year = {2005},
  __markedentry = {[george]},
  abstract = {We observed that for multimedia data ? especially music - collaborative
	similarity measures perform much better than similarity measures
	derived from content-based sound features. Our observation is based
	on a large scale evaluation with >250,000,000 collaborative data
	points crawled from the web and >190,000 songs annotated with content-based
	sound feature sets. A song mentioned in a playlist is regarded as
	one collaborative data point. In this paper we present a novel approach
	to bridging the performance gap between collaborative and content-
	based similarity measures. In the initial training phase a model
	vector for each song is computed, based on col- laborative data.
	Each vector consists of 200 overlapping unlabelled 'genres' or song
	clusters. Instead of using ex- plicit numerical voting, we use implicit
	user profile data as collaborative data source, which is, for example,
	available as purchase histories in many large scale e- commerce applications.
	After the training phase, we used support vector machines based on
	content-based sound features to predict the collaborative model vec-
	tors. These predicted model vectors are finally used to compute the
	similarity between songs. We show that combining collaborative and
	content-based similarity measures can help to overcome the new item
	problem in e-commerce applications that offer a collaborative simi-
	larity recommender as service to their customers.},
  owner = {george},
  pdf = {Stenzel2005.pdf},
  timestamp = {2007.03.20}
}

@ARTICLE{Sturm2006,
  author = {Bob L. Sturm},
  title = {Concatenative Sound Synthesis and Intellecual Property: An Analysis
	of the Legal Issues Surrounding the Synthesis of Novel Sounds from
	Copyright-Protected Work},
  journal = {Journal of New Music Research},
  year = {2006},
  volume = {35},
  pages = {23-33}
}

@INPROCEEDINGS{Tzanetakis2001,
  author = {George Tzanetakis and Georg Essl and Perry Cook},
  title = {Automatic Musical Genre Classification of Audio Signals},
  booktitle = {ISMIR},
  year = {2001},
  abstract = {Musical genres are categorical descriptions that are used to describe
	music. They are commonly used to structure the increasing amounts
	of music available in digital form on the Web and are important for
	music information retrieval. Genre categorization for audio has traditionally
	been performed manually. A particular musical genre is characterized
	by statistical properties related to the instrumentation, rhythmic
	structure and form of its members. In this work, algorithms for the
	automatic genre categorization of audio signals are described. More
	specifically, we propose a set of features for representing texture
	and instrumentation. In addition a novel set of features for representing
	rhythmic structure and strength is proposed. The performance of those
	feature sets has been evaluated by training statistical pattern recognition
	classifiers using real world audio collections. Based on the automatic
	hierarchical genre classification two graphical user interfaces for
	browsing and interacting with large audio collections have been developed.},
  owner = {george},
  pdf = {Tzanetakis2001.pdf},
  timestamp = {2007.03.20}
}

@INPROCEEDINGS{Uitenbogerd2002,
  author = {Alexandra L. Uitenbogerd and Justin Zobel},
  title = {Music Ranking Techniques Evaluated},
  booktitle = {Australiasian Computer Science Conference},
  year = {2002},
  pages = {275--283},
  address = {Melbourne, Australia},
  owner = {george},
  pdf = {http://www.seg.rmit.edu.au/research/download.php?manuscript=86},
  timestamp = {2007.04.15}
}

@MASTERSTHESIS{Vossen2005,
  author = {M. P. H. Vossen},
  title = {Local Search for Automatic Playlist Generation},
  school = {Technische Universitet Eindhoven: Department of Mathematics and Computing
	Science},
  year = {2005},
  month = {jun},
  owner = {george},
  pdf = {Vossen2005.pdf},
  timestamp = {2007.03.20}
}

@MISC{Whitby2006,
  author = {Blay Whitby},
  title = {Advanced Technical Communication: Writing an Abstract (Lecture Slides)},
  howpublished = {Online},
  month = {18 Feb},
  year = {2007},
  owner = {george},
  pdf = {Whitby2006.pdf},
  timestamp = {2007.04.10}
}

@MISC{Wikipedia2007,
  author = {Wikipedia},
  title = {Autocorrelation},
  howpublished = {Online},
  month = {24 Mar},
  year = {2007},
  note = {\url{http://en.wikipedia.org/en/Autocorrelation}},
  owner = {george},
  timestamp = {2007.04.15}
}

@INPROCEEDINGS{Wood2005,
  author = {Gavin Wood and Simon O'Keefe},
  title = {On Techniques for Content-Based Visual Annotation to Aid Intra-Track
	Music Navigation},
  booktitle = {ISMIR},
  year = {2005},
  pdf = {Wood2005.pdf}
}

@MISC{Copyright1988,
  title = {Copyright, Designs and Patents Act},
  year = {1988},
  chapter = {48}
}

