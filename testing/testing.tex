\newcommand{\graph}[2]{
\begin{figure}[!hp]
	\caption{#2}
	\includegraphics[width=\textwidth]{testing/graphs/#1}
	\label{graph:#1}
\end{figure}}
\section{Testing}
\label{text:testing}
Three approaches have been taken to the testing: functional testing of the system; automated testing of the affect of the four algorithms on the results; and user testing of performance.
\subsection{Automated Testing}
Once functional testing was completed (to ensure correct operation of the system), automated testing using the results of the music quiz was performed. For each result from the music quiz, the music library was sorted according to similarity to \emph{similar track A}; the system was then rated how high it placed \emph{similar track B} and how low it placed the \emph{not-similar track}.
\subsection{Results}
Figure \ref{graph:metric-comparison} shows a comparison of the four track comparison methods, before any training (all weights set to 1). It is clear from these results that the sorted comparison performed the best overall, having a higher average placement of the similar tracks, a relatively high difference between similar and not-similar placement, and a standard deviation in placement much less than that of the ordered area comparison.
\graph{metric-comparison}{Comparison of Similarity Metrics}

Testing of the training algorithm showed that more refinement was needed; after subsequent amendments, the results proved much better than both an untrained system and one trained with the old algorithm: a reduced mean and standard deviation of placement of similar track, and a much larger difference in similar and not-similar placement.
\graph{training-algorithm-comparison}{Comparison of Training Algorithms}

\section{User Testing}
\label{text:testing:user}
Three users with prior musical knowledge evaluated the system under the headings of similarity measure, playlist generation, and a blind-trialling test given. These evaluations highlighted the following:
\begin{itemize}
	\item Similarity Measure
	\begin{itemize}
		\item System had high accuracy when dealing with percussion, vocal style and instrumentation
		\item System tended to return songs of similar mood
		\item System dealt less well with tempo, due to the lack of tempo-related features
	\end{itemize}
	\item Playlist Generation
	\begin{itemize}
		\item Playlists flowed well in rhythm and instrumentation
		\item Long playlists caused dissimilar tracks to be played, due to the \emph{no repeated artists} rule
	\end{itemize}
	\item Blind-trialling of three playlists: random, manually created, and system generated
	\begin{itemize}
		\item All playlists correctly identified, users less secure in distinguishing system and manually created ones
		\item Random playlist quickly identified due to lack of coherence in genre; users found differentiation between other two playlists difficult due to similar properties
	\end{itemize}
\end{itemize}
\section{Critique of Testing Method}
%Work above 90\% will demonstrate understanding of the problems and limitations of evidence and arguments and the means by which they can be overcome
The testing methods held implications for more accurate results. The listening test could be modified to gain feedback from the user about \emph{how} similar the two tracks are, allowing refinement both of the training algorithm and the testing metric, comparing the expected and actual placement of tracks.

The two subjective tests of similarity measure and playlist generation are useful to gain feedback on flaws in the model of perceptual similarity measure and playlist generation, and provides a rough qualitative measure of the performance of the system. A set of playlists generated by expert users could be used to train the system based on empirical evidence; if this were split into a training and test set, a quantitative measure of the quality of a playlist could be developed.
